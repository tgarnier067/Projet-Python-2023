{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Définition du Problème et Objectifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexte et Pertinence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un contexte mondial où la gestion efficace de l'énergie est devenue une priorité cruciale, notre projet vise à analyser la consommation d'énergie par département dans différents secteurs en France. L'objectif est de fournir une compréhension approfondie des modèles de consommation énergétique, qui est essentielle pour orienter les politiques énergétiques, promouvoir la durabilité et optimiser les ressources. Ce projet est particulièrement pertinent étant donné les défis actuels liés au changement climatique et à la transition énergétique. En examinant les données de consommation énergétique à l'échelle des communes, nous pouvons identifier des tendances spécifiques, des anomalies et des opportunités d'amélioration. Cela permettra aux décideurs, aux entreprises et aux consommateurs de prendre des mesures éclairées pour réduire la consommation d'énergie, améliorer l'efficacité énergétique et favoriser l'adoption d'énergies renouvelables. En outre, ce projet contribue à une meilleure compréhension des disparités régionales en matière de consommation d'énergie, offrant ainsi une perspective précieuse pour des interventions ciblées et personnalisées.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs du Projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre projet, au cœur de l'intersection entre technologie, environnement et société, se fixe des objectifs ambitieux et significatifs :\n",
    "\n",
    "**Cartographie de la Consommation Énergétique :** Notre premier objectif est de dresser une carte précise de la consommation d'énergie dans les différents départements français. En mettant en lumière ces données, nous souhaitons offrir une vision claire et détaillée de la répartition énergétique sur le territoire.\n",
    "\n",
    "**Identification des Tendances et Anomalies :** Nous visons à décrypter les tendances sous-jacentes et à détecter d'éventuelles anomalies dans les habitudes de consommation énergétique. Cela permettra de comprendre les pratiques énergétiques actuelles et d'identifier les zones à haut potentiel d'amélioration.\n",
    "\n",
    "**Analyse Comparative par Secteur :** Un autre objectif crucial est de comparer la consommation énergétique entre différents secteurs (résidentiel, industriel, commercial, etc.). Cela aidera à cerner les secteurs les plus énergivores et à envisager des stratégies d'optimisation.\n",
    "\n",
    "**Prédiction des Tendances Futures :** Nous ambitionnons de développer des modèles prédictifs pour anticiper les évolutions futures de la consommation d'énergie. Ces prévisions seront essentielles pour planifier des stratégies énergétiques à long terme.\n",
    "\n",
    "**Contribution à la Durabilité :** En offrant une compréhension approfondie de la consommation d'énergie, le projet aspire à contribuer activement à des initiatives de développement durable. Les insights générés pourraient inspirer des actions concrètes pour réduire l'empreinte énergétique.\n",
    "\n",
    "**Support aux Décisions Politiques et Commerciales :** Fournir des données et des analyses fiables pour éclairer les décisions politiques et commerciales en matière de gestion de l'énergie. Cela inclut la recommandation de politiques efficaces et la sensibilisation aux meilleures pratiques en matière de consommation énergétique.\n",
    "\n",
    "**Sensibilisation et Éducation :** Enfin, nous souhaitons utiliser nos résultats pour sensibiliser le public et les décideurs aux enjeux de la consommation d'énergie. L'objectif est de promouvoir une culture de consommation énergétique responsable et informée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspects techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages\n",
    "\n",
    "%pip install -q lxml\n",
    "%pip install webdriver-manager\n",
    "%pip install BeautifulSoup4\n",
    "%pip install pandas fiona shapely pyproj rtree # à faire obligatoirement en premier pour utiliser rtree ou pygeos pour les jointures spatiales\n",
    "%pip install contextily\n",
    "%pip install geopandas\n",
    "%pip install pygeos\n",
    "%pip install topojson\n",
    "%pip install seaborn\n",
    "%pip install statsmodels\n",
    "%pip install scikit-learn\n",
    "%pip install numpy\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import requests\n",
    "import re\n",
    "import bs4\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1 = \"https://odre.opendatasoft.com/explore/embed/dataset/conso-departement-annuelle/table/?disjunctive.libelle_departement&disjunctive.libelle_region&disjunctive.e_operateurs&disjunctive.g_operateurs&refine.annee=2021&dataChart=eyJxdWVyaWVzIjpbeyJjaGFydHMiOlt7InR5cGUiOiJjb2x1bW4iLCJmdW5jIjoiQVZHIiwieUF4aXMiOiJjb25zb3RvdGFsZSIsInNjaWVudGlmaWNEaXNwbGF5Ijp0cnVlLCJjb2xvciI6IiM2NmMyYTUifV0sInhBeGlzIjoibGliZWxsZV9kZXBhcnRlbWVudCIsIm1heHBvaW50cyI6NTAsInNvcnQiOiIiLCJjb25maWciOnsiZGF0YXNldCI6ImNvbnNvLWRlcGFydGVtZW50LWFubnVlbGxlIiwib3B0aW9ucyI6eyJkaXNqdW5jdGl2ZS5saWJlbGxlX2RlcGFydGVtZW50Ijp0cnVlLCJkaXNqdW5jdGl2ZS5saWJlbGxlX3JlZ2lvbiI6dHJ1ZSwiZGlzanVuY3RpdmUuZV9vcGVyYXRldXJzIjp0cnVlLCJkaXNqdW5jdGl2ZS5nX29wZXJhdGV1cnMiOnRydWUsInJlZmluZS5hbm5lZSI6IjIwMjEifX19XSwidGltZXNjYWxlIjoiIiwiZGlzcGxheUxlZ2VuZCI6dHJ1ZSwiYWxpZ25Nb250aCI6dHJ1ZX0%3D&location=3,17.56025,53.4375&basemap=jawg.light\"\n",
    "url_2 = \"https://www.insee.fr/fr/statistiques/6436484?sommaire=6036904#tableau-figure1_radio1\"\n",
    "url_3 = \"https://odre.opendatasoft.com/explore/dataset/temperature-quotidienne-departementale/information/?disjunctive.departement\"\n",
    "url_4 = \"https://www.insee.fr/fr/statistiques/6436484?sommaire=6036904#tableau-figure1_radio1\"\n",
    "url_5 = \"https://www.observatoire-des-territoires.gouv.fr/outils/cartographie-interactive/#bbox=-1052198,6661338,2597056,1619174&c=indicator&i=insee_rp_hist_1968.part_logt_vacant&s=2020&view=map9\"\n",
    "url_6 = \"https://ufe-electricite.fr/watt-the-carte/deploiement-bornes-de-recharge-en-france/dans-les-territoires/\"\n",
    "url_7 = \"https://www.carburants.org/borne-electrique/departements/\"\n",
    "url_8 : \"https://www.observatoire-des-territoires.gouv.fr/nombre-dentreprises-par-secteurs-dactivite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collecte, nettoyage et préparation des données (Communes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases de données pre-existantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Consommation totale d'énergie par commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = https://www.data.gouv.fr/fr/datasets/consommation-annuelle-delectricite-et-gaz-par-commune-et-par-secteur-dactivite/\n",
    "table_conso_com = pd.read_csv('conso_energie.csv',sep=';')\n",
    "table_conso_com.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logements vacants par commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = https://www.data.gouv.fr/fr/datasets/niveau-de-vie-des-francais-par-commune/\n",
    "table_logements_vacants_com = pd.read_csv('logement_vacants_com.csv', sep=';', encoding='ISO-8859-1')\n",
    "table_logements_vacants_com.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nombre d'entreprises par commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = https://www.observatoire-des-territoires.gouv.fr/outils/cartographie-interactive/#c=indicator&f=TOT&i=demo_ent_sect.ent_tot&s=2021&view=map59\n",
    "table_nb_entr_com = pd.read_csv('table_nb_entr_com.csv',sep=';')\n",
    "table_nb_entr_com.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Niveau de vie par commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = https://www.observatoire-des-territoires.gouv.fr/outils/cartographie-interactive/#c=indicator&i=filosofi.med_disp&s=2020&view=map59\n",
    "table_niveau_vie_com = pd.read_csv('niveau_vie_com.csv',sep=';')\n",
    "table_niveau_vie_com.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Taux de déplacement domicile-travail en transport en commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = https://www.observatoire-des-territoires.gouv.fr/outils/cartographie-interactive/#bbox=-645836,6062176,1794690,1016154&c=indicator&i=insee_rp_hist_xxxx.part_domtrav_voit&s=2020&view=map59\n",
    "\n",
    "table_dep_domtrav_tc = pd.read_csv('dep_domtrav_tc.csv', sep=';', encoding='ISO-8859-1')\n",
    "table_dep_domtrav_tc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases de données webscrappées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Population par commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import re  # Importer le module pour les expressions régulières\n",
    "\n",
    "url_communes = \"https://fr.wikipedia.org/wiki/Listes_des_communes_de_France\"\n",
    "text_communes = request.urlopen(url_communes).read().decode('utf-8')\n",
    "page_communes = BeautifulSoup(text_communes, 'html.parser')\n",
    "tableau_communes = page_communes.find('table', {'class': 'wikitable'})\n",
    "tableau_communes = tableau_communes.find('tbody')\n",
    "lignes_communes = tableau_communes.find_all('tr')\n",
    "lignes_communes = lignes_communes[:-13]\n",
    "\n",
    "# Liste pour stocker les contenus après \"a href\"\n",
    "liste_url_communes = []\n",
    "\n",
    "# Parcourir chaque ligne dans lignes_communes\n",
    "for ligne in lignes_communes:\n",
    "    # Trouver toutes les balises <td> dans la ligne\n",
    "    td_tags = ligne.find_all('td')\n",
    "\n",
    "    if len(td_tags) >= 3:\n",
    "        if td_tags[-3].text.strip() != '75':\n",
    "            derniere_td = td_tags[-1]\n",
    "            a_tag = derniere_td.find('a')\n",
    "            if a_tag:\n",
    "                contenu_apres_href = a_tag.get('href')\n",
    "                liste_url_communes.append(contenu_apres_href)\n",
    "\n",
    "dico_communes = {}\n",
    "liste_code_communes = []\n",
    "\n",
    "for url in liste_url_communes:\n",
    "    text = request.urlopen(\"https://fr.wikipedia.org\" + url).read().decode('utf-8')\n",
    "    page = BeautifulSoup(text, 'html.parser')  # Utilisez html.parser au lieu de lxml\n",
    "    tableau = page.find('table', {'class': 'wikitable sortable titre-en-couleur'})\n",
    "    tableau = tableau.find('tbody')\n",
    "    lignes = tableau.find_all('tr')\n",
    "    lignes.pop(0)\n",
    "    lignes.pop(-1)\n",
    "\n",
    "    for ligne in lignes:\n",
    "        donnees = ligne.find_all('td')\n",
    "        code_insee = donnees[1].text.strip()\n",
    "        liste_code_communes.append(code_insee)\n",
    "        pop_commune = donnees[-3].text.strip()\n",
    "        dico_communes[code_insee] = pop_commune\n",
    "\n",
    "# Créer le DataFrame à partir du dictionnaire\n",
    "table_pop_com = pd.DataFrame.from_dict(dico_communes, orient='index').reset_index()\n",
    "table_pop_com = table_pop_com.rename(columns={'index': 'Code commune'})\n",
    "table_pop_com.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération de données via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Température par commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Dictionnaire pour stocker les moyennes de température par commune\n",
    "dico_temp_communes = {}\n",
    "\n",
    "root_api = \"https://public.opendatasoft.com\"\n",
    "\n",
    "# Initialiser un compteur pour les valeurs manquantes\n",
    "missing_values_count = 0\n",
    "\n",
    "# Supposons que liste_code_communes soit définie ailleurs dans votre code\n",
    "# liste_code_communes = [...]\n",
    "\n",
    "for codegeo in liste_code_communes:\n",
    "    # Arrêter le programme si on a trop de valeurs manquantes\n",
    "    if missing_values_count >= 100:\n",
    "        print(\"On arrête le programme en raison d'un nombre de valeurs manquantes trop important\")\n",
    "        break\n",
    "\n",
    "    url = f\"{root_api}/api/explore/v2.1/catalog/datasets/donnees-synop-essentielles-omm/records?select=codegeo%2C%20tc%2C%20latitude%2C%20longitude&where=codegeo%3D%22{codegeo}%22&limit=99\"\n",
    "    req = requests.get(url)\n",
    "    temp = req.json()\n",
    "    results = temp.get('results', [])\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    if 'tc' in df.columns:\n",
    "        # Compter le nombre de valeurs manquantes dans la colonne 'tc'\n",
    "        missing_values_count += df['tc'].isna().sum()\n",
    "\n",
    "        # Calculer la moyenne des températures, en ignorant les valeurs manquantes\n",
    "        moyenne = round(df['tc'].mean(), 2)\n",
    "        dico_temp_communes[codegeo] = moyenne\n",
    "    else:\n",
    "        missing_values_count += 1  # Compter l'absence complète de la colonne comme une valeur manquante\n",
    "        print(f\"La colonne 'tc' est absente pour le codegeo {codegeo}\")\n",
    "\n",
    "# Si le programme s'achève sans interruption, afficher les résultats\n",
    "if missing_values_count < 100:\n",
    "    print(dico_temp_communes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se rend compte qu'on a plein de données manquantes. Pour lutter contre ce problème on associe à chaque commune la température du département, en faisant l'hypothèse que la température est assez homogène dans un département."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de base et structure de l'URL de l'API\n",
    "root_api = \"https://odre.opendatasoft.com\"\n",
    "base_url = \"/api/explore/v2.1/catalog/datasets/temperature-quotidienne-departementale/records\"\n",
    "base_query = \"?select=date_obs%2Ccode_insee_departement%2Cdepartement%2Ctmoy&order_by=code_insee_departement&limit=99&refine=date_obs%3A%222021%22\"\n",
    "\n",
    "# Collecte des données pour chaque mois\n",
    "df_list = []\n",
    "for i in range(1, 13):\n",
    "    date_str = f\"2021-{i:02d}-01\"\n",
    "    url_api = f\"{root_api}{base_url}{base_query}&where=date_obs%3Ddate'{date_str}'\"\n",
    "    req = requests.get(url_api)\n",
    "    temp = req.json()\n",
    "    results = temp.get('results', [])\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        df = df[['date_obs', 'code_insee_departement', 'departement','tmoy']]\n",
    "        df_list.append(df)\n",
    "\n",
    "# Fusionner tous les DataFrames en un seul\n",
    "df_final = pd.concat(df_list)\n",
    "\n",
    "# Calcul de la moyenne des températures par code et nom de département\n",
    "table_temperatures = df_final.groupby(['code_insee_departement', 'departement'])['tmoy'].mean().reset_index()\n",
    "\n",
    "# Affichage du DataFrame final\n",
    "table_temperatures.head()\n",
    "\n",
    "# Création d'un DataFrame à partir de la liste des codes INSEE\n",
    "data_communes = {'code_commune': liste_code_communes}\n",
    "df_communes = pd.DataFrame(data_communes)\n",
    "\n",
    "# Exécution d'une jointure entre les deux DataFrames\n",
    "table_temperatures_com = pd.merge(df_communes, table_temperatures, left_on=df_communes['code_commune'].str[:2], right_on=table_temperatures['code_insee_departement'])\n",
    "\n",
    "# Afficher le résultat\n",
    "table_temperatures_com[['code_commune', 'tmoy']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation des dataframe a fusionner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes nécessaires\n",
    "df_filtered = table_conso_com[['annee', 'filiere', 'code_commune', 'libelle_commune', 'consototale']]\n",
    "\n",
    "# Filtrer les données pour ne garder que celles de l'année 2021, et l'electricité\n",
    "df_filtered = df_filtered[df_filtered['annee'] == 2021]\n",
    "df_filtered = df_filtered[df_filtered['filiere'] == 'Electricité']\n",
    "\n",
    "# Convertir la colonne 'code_commune' en chaîne de caractères\n",
    "df_filtered['code_commune'] = df_filtered['code_commune'].astype(str)\n",
    "\n",
    "# Trier les données par commune en ordre croissant\n",
    "df_filtered = df_filtered.sort_values(by=['code_commune'])\n",
    "\n",
    "# Garder la première occurrence pour chaque commune\n",
    "df_filtered = df_filtered.drop_duplicates(subset='code_commune')\n",
    "\n",
    "# Réinitialiser l'index\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "# Affecter le DataFrame final à la variable 'table_conso_com'\n",
    "table_conso_com = df_filtered\n",
    "table_conso_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_temperatures_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_logements_vacants_com=table_logements_vacants_com.rename(columns={\n",
    "    'codgeo': 'code_commune',\n",
    "    'libgeo': 'libelle_commune',\n",
    "    'an': 'an',\n",
    "    'part_logt_vacant': 'logements_vacants_%'\n",
    "})\n",
    "table_logements_vacants_com = table_logements_vacants_com[table_logements_vacants_com['an'] == 2020]\n",
    "table_logements_vacants_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_pop_com=table_pop_com.rename(columns={\n",
    "    'Code commune': 'code_commune',\n",
    "    0: 'population',\n",
    "})\n",
    "table_pop_com['population'] = table_pop_com['population'].str.replace(r'\\s*\\([^)]*\\)\\s*', '', regex=True)\n",
    "table_pop_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_niveau_vie_com=table_niveau_vie_com.rename(columns={\n",
    "    'Code': 'code_commune',\n",
    "    'Libellé': 'libelle_commune',\n",
    "    'Médiane du revenu disponible par UC 2020': 'niveau_de_vie'\n",
    "})\n",
    "table_niveau_vie_com['niveau_de_vie'] = table_niveau_vie_com['niveau_de_vie'].replace('N/A - résultat non disponible', pd.NA)\n",
    "table_niveau_vie_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_nb_entr_com = table_nb_entr_com.rename(columns={\n",
    "    'Code': 'code_commune',\n",
    "    'Libellé': 'libelle_commune',\n",
    "    'Nombre d\\'entreprises par secteurs d\\'activité 2021': 'nombre_entreprises',\n",
    "    'Nombre d\\'entreprises par secteurs d\\'activité 2021.1': 'pas_compris_cette_colonne'\n",
    "})\n",
    "table_nb_entr_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dep_domtrav_tc = table_dep_domtrav_tc.rename(columns={\n",
    "    'codgeo':'code_commune',\n",
    "    'libgeo':'libelle_commune',\n",
    "    'an':'an',\n",
    "    'part_domtrav_tc':'taux_deplacement_domicile_travail'\n",
    "})\n",
    "table_dep_domtrav_tc = table_dep_domtrav_tc[table_dep_domtrav_tc['an'] == 2020]\n",
    "table_dep_domtrav_tc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Traitement des tableaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_conso_communes=table_conso_com[['code_commune','libelle_commune','consototale']]\n",
    "table_logements_vacants_communes = table_logements_vacants_com.loc[table_logements_vacants_com.groupby('code_commune')['an'].idxmax()]\n",
    "table_nb_entr_communes=table_nb_entr_com[['code_commune','libelle_commune','nombre_entreprises']]\n",
    "table_niveau_vie_communes=table_niveau_vie_com[['code_commune','libelle_commune','niveau_de_vie']]\n",
    "table_pop_communes=table_pop_com[['code_commune','population']]\n",
    "table_temperatures_communes=table_temperatures_com[['code_commune','tmoy']]\n",
    "table_dep_domtrav_tc=table_dep_domtrav_tc[['code_commune','taux_deplacement_domicile_travail']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Harmonisation du format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des DataFrames\n",
    "dataframes = [table_conso_communes, table_logements_vacants_communes, table_nb_entr_communes, table_niveau_vie_communes, table_pop_communes, table_temperatures_communes, table_dep_domtrav_tc]\n",
    "\n",
    "# Boucle à travers les DataFrames\n",
    "for df in dataframes:\n",
    "    # Convertir la colonne 'code_commune' en chaîne de caractères\n",
    "    df['code_commune'] = df['code_commune'].astype(str)\n",
    "    \n",
    "    # Ajouter des zéros à gauche pour avoir 5 chiffres\n",
    "    df['code_commune'] = df['code_commune'].str.zfill(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fusion deux par deux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion des DataFrames\n",
    "df_merged = table_conso_communes.merge(table_logements_vacants_communes, on='code_commune', how='outer')\n",
    "df_merged = df_merged.merge(table_nb_entr_communes, on='code_commune', how='outer', suffixes=('', '_entr'))\n",
    "df_merged = df_merged.merge(table_niveau_vie_communes, on='code_commune', how='outer', suffixes=('', '_nv'))\n",
    "df_merged = df_merged.merge(table_pop_communes, on='code_commune', how='outer')\n",
    "df_merged = df_merged.merge(table_temperatures_communes, on='code_commune', how='outer')\n",
    "df_merged = df_merged.merge(table_dep_domtrav_tc, on='code_commune', how='outer')\n",
    "\n",
    "# Nettoyage des colonnes\n",
    "table_donnees = df_merged[['code_commune', 'libelle_commune', 'consototale', 'logements_vacants_%', 'nombre_entreprises', 'niveau_de_vie', 'population', 'tmoy','taux_deplacement_domicile_travail']]\n",
    "table_donnees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Liste des variables explicatives\n",
    "variables_modele = ['consototale', 'logements_vacants_%', 'nombre_entreprises', 'niveau_de_vie', 'population', 'tmoy', 'taux_deplacement_domicile_travail']\n",
    "\n",
    "# Convertir les colonnes en type chaîne\n",
    "table_donnees_test = table_donnees[variables_modele].astype(str)\n",
    "\n",
    "# Nettoyer les colonnes avec des virgules et convertir en float\n",
    "for variable in variables_modele:\n",
    "    table_donnees_test[variable] = table_donnees_test[variable].str.replace(',', '.')\n",
    "    table_donnees_test[variable] = table_donnees_test[variable].str.replace('\\xa0', ' ')\n",
    "    table_donnees_test[variable] = table_donnees_test[variable].str.replace(' ', '')\n",
    "    table_donnees_test[variable] = pd.to_numeric(table_donnees_test[variable], errors='coerce')  # Convertir en float avec gestion des erreurs\n",
    "\n",
    "# Remplacez les colonnes d'origine par les colonnes nettoyées et converties\n",
    "table_donnees[variables_modele] = table_donnees_test\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "table_donnees.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conversion des colonnes en chaînes de caractères et remplacement des virgules et espaces insécables\n",
    "table_donnees['logements_vacants_%'] = table_donnees['logements_vacants_%'].astype(str).str.replace(',', '.').astype(float)\n",
    "table_donnees['nombre_entreprises'] = table_donnees['nombre_entreprises'].astype(str).str.replace(',', '.').astype(float)\n",
    "table_donnees['niveau_de_vie'] = table_donnees['niveau_de_vie'].replace('N/A - résultat non disponible', pd.NA)\n",
    "#table_donnees['population'] = table_donnees['population'].astype(str).str.replace('\\xa0', '').replace(' ', '').astype(int)\n",
    "\n",
    "# Vérification des conversions\n",
    "print(table_donnees.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyse et Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiques descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats générales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives pour les colonnes numériques\n",
    "stats_descriptives = table_donnees.describe()\n",
    "\n",
    "# Mode pour les colonnes catégorielles et numériques\n",
    "mode = table_donnees.mode().iloc[0]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nStatistiques descriptives :\\n\", stats_descriptives)\n",
    "print(\"\\nMode :\\n\", mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap améliorée pour la matrice de corrélation\n",
    "data_numerique = table_donnees.select_dtypes(include=[np.number])\n",
    "# Calcul de la matrice de corrélation sur les données numériques\n",
    "corr = data_numerique.corr()\n",
    "# Création de la heatmap\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', mask=mask)\n",
    "plt.title('Matrice de corrélation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Configuration du style des graphiques\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Histogrammes améliorés pour les variables numériques\n",
    "for col in ['consototale', 'logements_vacants_%', 'nombre_entreprises', 'niveau_de_vie', 'population', 'tmoy']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(table_donnees[col], kde=True, color=\"skyblue\", edgecolor='black', bins=30)\n",
    "    plt.title(f'Distribution de {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Fréquence')\n",
    "    plt.show()\n",
    "\n",
    "# Boxplots améliorés pour les variables numériques\n",
    "for col in ['consototale', 'logements_vacants_%', 'nombre_entreprises', 'niveau_de_vie', 'population', 'tmoy']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=table_donnees[col], palette=\"Set2\")\n",
    "    plt.title(f'Boxplot de {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Scatter plots améliorés pour les relations entre deux variables numériques\n",
    "# Exemple : 'population' vs 'consototale'\n",
    "plt.figure(figsize=(10, 6))\n",
    "#sns.regplot(x='population', y='consototale', data=table_donnees, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "plt.title('Relation entre Population et Consommation Totale')\n",
    "plt.xlabel('Population')\n",
    "plt.ylabel('Consommation Totale')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus sur la température"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max min\n",
    "Mt=table_donnees['tmoy'].max()\n",
    "mt=table_donnees['tmoy'].min()\n",
    "#on distingue trois groupe de dépapartement selon la température pour observer les causalités et les correlations.\n",
    "table_donnees['grp_tmp']=[ \"Chaud\" if t > 2*(Mt-mt)/3 +mt  else (\"Froid\" if t < (Mt-mt)/3 +mt else \"Doux\") for t in table_donnees['tmoy']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=table_donnees, y=\"consototale\", x=\"tmoy\", hue=\"grp_tmp\", height=6, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=table_donnees, hue=\"grp_tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse géographique : cartes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests py7zr geopandas openpyxl tqdm s3fs PyYAML xlrd\n",
    "%pip install git+https://github.com/inseefrlab/cartiflette@80b8a5a28371feb6df31d55bcc2617948a5f9b1a\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fond de carte metropole commune\n",
    "import cartiflette.s3 as s3\n",
    "metropole_communes = s3.download_vectorfile_url_all(\n",
    "      values = \"metropole\",\n",
    "      crs = 4326,\n",
    "      borders = \"COMMUNE\",\n",
    "      vectorfile_format=\"topojson\",\n",
    "      filter_by=\"FRANCE_ENTIERE\",\n",
    "      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n",
    "      year=2022)\n",
    "metropole_communes.rename(columns={'INSEE_COM': 'code_commune'}, inplace=True)\n",
    "metropole_communes = metropole_communes.drop('territoire', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fond de carte Dom TOM\n",
    "import cartiflette.s3 as s3\n",
    "dom_tom_communes = s3.download_vectorfile_url_all(\n",
    "      values = [\"971\",\"972\",\"973\",\"974\"],\n",
    "      crs = 4326,\n",
    "      borders = \"COMMUNE\",\n",
    "      vectorfile_format=\"topojson\",\n",
    "      filter_by=\"DEPARTEMENT\",\n",
    "      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n",
    "      year=2022)\n",
    "dom_tom_communes.rename(columns={'INSEE_COM': 'code_commune'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#france_communes=pd.concat([metropole_communes, dom_tom_communes], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carte_temp=pd.merge(table_temperatures_com[['code_commune', 'tmoy']],metropole_communes[[\"code_commune\",\"geometry\"]], on=\"code_commune\", how= 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation de la temperature par département\n",
    "#import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "#gdf = gpd.GeoDataFrame(carte_temp, geometry='geometry')\n",
    "#Créez une figure et des axes\n",
    "#fig, ax = plt.subplots(figsize=(30,30))\n",
    "# Tracé de la carte avec la couleur basée sur les températures \n",
    "#gdf.plot(column='tmoy', cmap='coolwarm', linewidth=0.05, ax=ax, edgecolor='0.8')\n",
    "# Ajout d'une barre de couleur :\n",
    "#divider = make_axes_locatable(ax)\n",
    "#cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n",
    "#sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=gdf['tmoy'].min(), vmax=gdf['tmoy'].max()))\n",
    "#sm._A = []\n",
    "#cbar = plt.colorbar(sm, cax=cax)\n",
    "\n",
    "# Suppression des axes\n",
    "#ax.set_axis_off()\n",
    "\n",
    "# Ajouter un titre à la carte\n",
    "#plt.suptitle(\"Carte de France - Températures des départements\", fontsize=15, x=0.5, y=0.80)\n",
    "\n",
    "# Afficher la carte\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation de la temperature par département DOM TOM ET METROPLE SUR LA MEME CARTE\n",
    "#import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "#pip install --upgrade geopandas matplotlib\n",
    "\n",
    "#temp_france=pd.merge(table_temperatures_com[['code_commune', 'tmoy']],metropole_communes[[\"code_commune\",\"geometry\"]], on=\"code_commune\", how= 'inner')\n",
    "#temp_dom_tom =pd.merge(table_temperatures_com[['code_commune', 'tmoy']],dom_tom_communes[[\"code_commune\",\"geometry\"]], on=\"code_commune\", how= 'inner')\n",
    "#fr = gpd.GeoDataFrame(temp_france, geometry='geometry')\n",
    "#dt = gpd.GeoDataFrame(temp_dom_tom, geometry='geometry')\n",
    "#dt = dt.to_crs(fr.crs)\n",
    "#Créez une figure et des axes\n",
    "#fig, ax = plt.subplots(figsize=(10,10))\n",
    "# Tracé de la carte avec la couleur basée sur les températures \n",
    "#fr.plot(column='tmoy', cmap='coolwarm', linewidth=0.05, ax=ax, edgecolor='0.8')\n",
    "#dt.plot(column='tmoy', cmap='coolwarm', linewidth=0.05, ax=ax, edgecolor='0.8')\n",
    "\n",
    "# Ajout d'une barre de couleur :\n",
    "#divider = make_axes_locatable(ax)\n",
    "#cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n",
    "#sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=gdf['tmoy'].min(), vmax=gdf['tmoy'].max()))\n",
    "#sm._A = []\n",
    "#cbar = plt.colorbar(sm, cax=cax)\n",
    "\n",
    "# Suppression des axes\n",
    "#ax.set_axis_off()\n",
    "\n",
    "# Ajouter un titre à la carte\n",
    "#plt.suptitle(\"Carte de France - Températures des départements\", fontsize=15, x=0.5, y=0.80)\n",
    "\n",
    "# Afficher la carte\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enlever les communes avec une consommaition totale de 0 \n",
    "print(len(table_donnees))\n",
    "df= table_donnees.copy()\n",
    "df = df[df['consototale'] != 0]\n",
    "print(len(df[\"consototale\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carte_temp=pd.merge(df,metropole_communes[[\"code_commune\",\"geometry\"]], on=\"code_commune\", how= 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation de la temperature par département/commune\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "gdf = gpd.GeoDataFrame(carte_temp, geometry='geometry')\n",
    "#Créez une figure et des axes\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "# Tracé de la carte avec la couleur basée sur les températures \n",
    "gdf.plot(column='tmoy', cmap='coolwarm', linewidth=0.05, ax=ax, edgecolor='0.8')\n",
    "# Ajout d'une barre de couleur :\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n",
    "sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=gdf['tmoy'].min(), vmax=gdf['tmoy'].max()))\n",
    "sm._A = []\n",
    "cbar = plt.colorbar(sm, cax=cax)\n",
    "\n",
    "# Suppression des axes\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Ajouter un titre à la carte\n",
    "plt.suptitle(\"Carte de France - Températures des communes\", fontsize=15, x=0.5, y=0.80)\n",
    "\n",
    "# Afficher la carte\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carte_conso=pd.merge(df,metropole_communes[[\"code_commune\",\"geometry\"]], on=\"code_commune\", how= 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consomation par commune\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import geopandas as gpd\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "\n",
    "gdf2 = gpd.GeoDataFrame(carte_conso, geometry='geometry')\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=gdf2['consototale'].min(), vcenter=gdf2['consototale'].mean(), vmax=200000)\n",
    "\n",
    "# Tracé de la carte avec la couleur basée sur les températures\n",
    "gdf2.plot(column='consototale', cmap='RdYlGn_r', linewidth=0.5, ax=ax, edgecolor='0.8', norm=norm)\n",
    "\n",
    "# Ajout d'une barre de couleur :\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"2%\", pad=0.5)\n",
    "\n",
    "# Utilisez la colonne normalisée pour la barre de couleur\n",
    "sm = plt.cm.ScalarMappable(cmap='RdYlGn_r', norm=norm)\n",
    "sm._A = []\n",
    "cbar = plt.colorbar(sm, cax=cax)\n",
    "\n",
    "# Suppression des axes\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Ajouter un titre à la carte\n",
    "plt.suptitle(\"Carte de France - Consommation des communes\", fontsize=15, x=0.5, y=0.80)\n",
    "\n",
    "# Afficher la carte\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mais un probleme est l'effet taille, la consommation est forcément croissante du nombre d'habitant donc il conviendrait de calculer la consommation par tête"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"consommation_par_tête\"] = df[\"consototale\"]/df[\"population\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consomation par tête, communes\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import geopandas as gpd\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "carte_conso_tete=pd.merge(df,metropole_communes[[\"code_commune\",\"geometry\"]], on=\"code_commune\", how= 'inner')\n",
    "\n",
    "gdf3 = gpd.GeoDataFrame(carte_conso_tete, geometry='geometry')\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=gdf3['consommation_par_tête'].min(), vcenter=gdf3['consommation_par_tête'].mean(), vmax=40)\n",
    "\n",
    "# Tracé de la carte avec la couleur basée sur les températures\n",
    "gdf3.plot(column='consommation_par_tête', cmap='RdYlGn_r', linewidth=0.5, ax=ax, edgecolor='0.8', norm=norm)\n",
    "\n",
    "# Ajout d'une barre de couleur :\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"2%\", pad=0.5)\n",
    "\n",
    "# Utilisez la colonne normalisée pour la barre de couleur\n",
    "sm = plt.cm.ScalarMappable(cmap='RdYlGn_r', norm=norm)\n",
    "sm._A = []\n",
    "cbar = plt.colorbar(sm, cax=cax)\n",
    "\n",
    "# Suppression des axes\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Ajouter un titre à la carte\n",
    "plt.suptitle(\"Carte de France - Consommation par tête des communes\", fontsize=15, x=0.5, y=0.80)\n",
    "\n",
    "# Afficher la carte\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelles conclusions, consommation dans les alpes plus forte "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modélisation et Prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ère idée : régression linéaire sur toutes les variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dans un premier temps, on standardise les données, cette méthode présente de nombreux avantages tels que :\n",
    "\n",
    "##### Stabilité numérique : Certains algorithmes, en particulier ceux qui utilisent des calculs matriciels, peuvent être sensibles à l'échelle des variables. La standardisation aide à éviter des problèmes numériques tels que la divergence ou la convergence lente des algorithmes.\n",
    "\n",
    "##### Comparaison directe des coefficients : Dans les modèles qui impliquent des coefficients (comme les modèles linéaires), la standardisation permet de comparer directement l'importance relative des variables en fonction de la taille de leurs coefficients. Cela peut rendre l'interprétation du modèle plus facile.\n",
    "\n",
    "##### Amélioration de la convergence : Certains algorithmes d'optimisation utilisés dans les modèles de machine learning convergent plus rapidement sur des données standardisées, ce qui peut accélérer le processus d'entraînement.\n",
    "\n",
    "##### Gestion des différences d'échelle : Lorsque les variables ont des échelles différentes, certaines d'entre elles peuvent dominer l'influence du modèle. La standardisation équilibre ces différences d'échelle et donne une importance similaire à toutes les variables lors de l'apprentissage du modèle.\n",
    "\n",
    "##### Prévention du surajustement : Certains algorithmes, tels que les SVM (Support Vector Machines) et les k-NN (k-Nearest Neighbors), sont sensibles à l'échelle des variables. La standardisation peut aider à prévenir le surajustement en équilibrant l'influence des différentes caractéristiques.\n",
    "\n",
    "##### Interprétabilité améliorée : La standardisation facilite l'interprétation des coefficients dans les modèles linéaires. Les coefficients standardisés indiquent combien d'écart-type change la variable explicative pour un changement d'une unité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_donnees=table_donnees.dropna()\n",
    "table_donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création matrice de correlation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcul de la matrice de corrélation\n",
    "matrice_correlation = table_donnees[variables_modele].corr()\n",
    "\n",
    "# Configuration de la palette de couleurs\n",
    "palette = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "# Création de la heatmap avec Seaborn et attribution à une variable\n",
    "heatmap = sns.heatmap(matrice_correlation, annot=True, cmap=palette, vmin=-1, vmax=1)\n",
    "\n",
    "# Affichage des noms de variables en haut de la matrice\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Affichage de la matrice de corrélation\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la table de corrélation : \n",
    "\n",
    "# On remarque sur la première ligne de la matrice, la corrélation entre la variable explicative, et chacune des variables expliquées.\n",
    "# On observe une correlation presque nulle entre la population et respectivement le taux de logements vacants, le niveau de vie et la température moyenne.\n",
    "# En revanche, on observe une forte corrélation empirique entre la consommation totale et respectivement le nombre d'entreprises, la taille de la population, et le taux de déplacement domocile travail en transports en commun. Ces variables sont potentiellement importantes pour le modèle de régression.\n",
    "\n",
    "# On peut s'attendre à ce que les variables faiblements correlées à la variable explicative ne soient pas significatives dans le modèle ?\n",
    "\n",
    "# Enfin, on observe une très forte corrélation, presque parfaite, entre le nombre d'entreprises et la population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Supposons que \"table_donnees\" est votre DataFrame initial et \"variables_modele\" sont les colonnes à standardiser\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Créez une copie du DataFrame initial\n",
    "df_standardise = table_donnees.copy()\n",
    "\n",
    "# Standardisez les colonnes spécifiées\n",
    "df_standardise[variables_modele] = scaler.fit_transform(df_standardise[variables_modele])\n",
    "\n",
    "# Sélectionnez les colonnes non standardisées\n",
    "colonnes_non_standardisees = [col for col in df_standardise.columns if col not in variables_modele]\n",
    "\n",
    "# Réorganisez les colonnes pour avoir les non standardisées au début suivies des standardisées\n",
    "df_standardise = df_standardise[colonnes_non_standardisees + variables_modele]\n",
    "\n",
    "df_standardise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Création de l'histogramme avant la standardisation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(table_donnees['logements_vacants_%'], bins=40, color='blue', edgecolor='black')\n",
    "plt.title('Histogramme avant standardisation')\n",
    "plt.xlabel('Valeurs')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.xlim([0, table_donnees['logements_vacants_%'].max()])\n",
    "\n",
    "# Création de l'histogramme après la standardisation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_standardise['logements_vacants_%'], bins=40, color='green', edgecolor='black')\n",
    "plt.title('Histogramme après standardisation')\n",
    "plt.xlabel('Valeurs standardisées')\n",
    "plt.ylabel('Fréquence')\n",
    "\n",
    "plt.xlim([-df_standardise['logements_vacants_%'].max(), df_standardise['logements_vacants_%'].max()])# Affichage des deux histogrammes# Affichage des deux histogrammes\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Création de l'histogramme avant la standardisation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(table_donnees['logements_vacants_%'], bins=40, color='blue', edgecolor='black')\n",
    "plt.title('Histogramme avant standardisation')\n",
    "plt.xlabel('Valeurs')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.xlim([-table_donnees['logements_vacants_%'].max(), table_donnees['logements_vacants_%'].max()])\n",
    "\n",
    "# Création de l'histogramme après la standardisation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_standardise['logements_vacants_%'], bins=40, color='green', edgecolor='black')\n",
    "plt.title('Histogramme après standardisation')\n",
    "plt.xlabel('Valeurs standardisées')\n",
    "plt.ylabel('Fréquence')\n",
    "\n",
    "plt.xlim([-table_donnees['logements_vacants_%'].max(), table_donnees['logements_vacants_%'].max()])\n",
    "\n",
    "# Affichage des deux histogrammes\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle et interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une base test et d'une base d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparation en variables indépendantes (X) et variable dépendante (y)\n",
    "X = df_standardise.drop(['code_commune','libelle_commune','consototale','grp_tmp'], axis=1)\n",
    "y = df_standardise['consototale']\n",
    "\n",
    "# Division des données en ensemble d'entraînement (train set) et ensemble de test (test set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# \"test_size\" détermine la proportion d'observations à inclure dans l'ensemble de test (ici, 20%)\n",
    "# \"random_state\" est utilisé pour garantir la reproductibilité des résultats, on peut choisir n'importe quel nombre\n",
    "\n",
    "# Affichage des dimensions des ensembles d'entraînement et de test\n",
    "print(\"Dimensions de l'ensemble d'entraînement (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Dimensions de l'ensemble de test (X_test, y_test):\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Régression Linéaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Ajouter une constante à la matrice des caractéristiques (X) pour le terme d'interception\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Création d'un modèle de régression linéaire avec statsmodels\n",
    "modele_regression = sm.OLS(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interprétation / significativité des coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_regression.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interprétation des coefficients : \n",
    "\n",
    "## Logements_vacants_% :\n",
    "# Le coefficient de cette variable est : -0.0084. Ce coefficient est faible, en valeur absolue, et relativement aux autres coefficients : il semblerait donc que cette variable explique peu la consommation d'énergie\n",
    "# Son signe est négatif , il semblerait donc qu'augmenter le taux de logements vacants dans une commune fasse diminuer la consommation totale d'énergie.\n",
    "# Sa p-value étant de 0.133 , il semblerait que cette variable ne soit pas significative, même au seuil de 10%.\n",
    "\n",
    "## nombre_entreprises :\n",
    "# Le coefficient de cette variable est : 0.0199 Ce coefficient est faible, en valeur absolue, et relativement aux autres coefficients : il semblerait donc que cette variable explique peu la consommation d'énergie\n",
    "# Son signe est positif , il semblerait donc qu'augmenter le nombre d'entreprises dans une commune fasse augmenter la consommation totale d'énergie.\n",
    "# Sa p-value étant de 0.237 , il semblerait que cette variable ne soit pas significative, même au seuil de 10%\n",
    "\n",
    "## niveau_de_vie :\n",
    "# Le coefficient de cette variable est : -0.0090. Ce coefficient est faible, en valeur absolue, et relativement aux autres coefficients : il semblerait donc que cette variable explique peu la consommation d'énergie\n",
    "# Son signe est negatif , il semblerait donc qu'augmenter le niveau de vie dans une commune fasse diminuer la consommation totale d'énergie.\n",
    "# Sa p-value étant de 0.121 , il semblerait que cette variable ne soit pas significative, même au seuil de 10%\n",
    "\n",
    "## population :\n",
    "# Le coefficient de cette variable est : 0.5185. Ce coefficient est élevé, en valeur absolue, et relativement aux autres coefficients : il semblerait donc que cette variable explique beaucoup la consommation d'énergie\n",
    "# Son signe est positif , il semblerait donc qu'augmenter la population dans une commune fasse augmenter la consommation totale d'énergie.\n",
    "# Sa p-value étant de 0.000 , il semblerait que cette variable soit significative, même au seuil de 1%\n",
    "\n",
    "## tmoy :\n",
    "# Le coefficient de cette variable est : 0.0148. Ce coefficient est faible, en valeur absolue, et relativement aux autres coefficients : il semblerait donc que cette variable explique peu la consommation d'énergie\n",
    "# Son signe est positif , il semblerait donc qu'augmenter la température moyenne dans un département fasse augmenter la consommation totale d'énergie des communes.\n",
    "# Sa p-value étant de 0.005 , il semblerait que cette variable soit significative, même au seuil de 1%\n",
    "\n",
    "## taux_deplacement_domicile_travail :\n",
    "# Le coefficient de cette variable est : 0.0800. Ce coefficient est faible, en valeur absolue, et relativement aux autres coefficients : il semblerait donc que cette variable explique peu la consommation d'énergie\n",
    "# Son signe est poisitif , il semblerait donc qu'augmenter le taux de déplacements domicile-travail en transports en commun dans une commune fasse augmenter la consommation totale d'énergie\n",
    "# Sa p-value étant de 0.000 , il semblerait que cette variable soit significative, même au seuil de 1%\n",
    "\n",
    "\n",
    "# Pour conclure sur l'analyse des coefficients de ce modèle, on observe des signes de coefficients qui vont à l'encontre de nos hypothèses : \n",
    "# Nous avions supposé que les relations entre le taux de logements vacants, ainsi que le niveau de vie, avec la consommation d'energie était positives. La régression semble nous indiquer le contraire.\n",
    "# Parallèlement, Nous avions supposé une relation négative entre la température moyenne et la consommation d'energie. Encore une fois, la régression semble nous indiquer le contraire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation de la qualité de prédiction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant qu'on a interprété les coefficients, on revient à des données non standardisées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparation en variables indépendantes (X) et variable dépendante (y)\n",
    "X = table_donnees.drop(['code_commune','libelle_commune','consototale','grp_tmp'], axis=1)\n",
    "y = table_donnees['consototale']\n",
    "\n",
    "# Division des données en ensemble d'entraînement (train set) et ensemble de test (test set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# \"test_size\" détermine la proportion d'observations à inclure dans l'ensemble de test (ici, 20%)\n",
    "# \"random_state\" est utilisé pour garantir la reproductibilité des résultats, on peut choisir n'importe quel nombre\n",
    "\n",
    "# Affichage des dimensions des ensembles d'entraînement et de test\n",
    "print(\"Dimensions de l'ensemble d'entraînement (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Dimensions de l'ensemble de test (X_test, y_test):\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Ajouter une constante à la matrice des caractéristiques (X) pour le terme d'interception\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Création d'un modèle de régression linéaire avec statsmodels\n",
    "modele_regression = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "modele_regression.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker \n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "predictions = modele_regression.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On observe un R2 relativement faible => qualité prédictive de notre modèle est mauvaise mais on peut chercher à l'améliorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une disposition de sous-tracés 1x3\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(y_test, predictions, label='Prédictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--k', label='y=x', linewidth=2)\n",
    "plt.xlabel(\"Vraies valeurs\")\n",
    "plt.ylabel(\"Prédictions\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(y_test, predictions, label='Prédictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--k', label='y=x', linewidth=2)\n",
    "plt.xlabel(\"Vraies valeurs\")\n",
    "plt.ylabel(\"Prédictions\")\n",
    "plt.xlim(-30000, 500000)\n",
    "plt.ylim(-30000, 500000)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(y_test, predictions, label='Prédictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--k', label='y=x', linewidth=2)\n",
    "plt.xlabel(\"Vraies valeurs\")\n",
    "plt.ylabel(\"Prédictions\")\n",
    "plt.xlim(-10000, 200000)\n",
    "plt.ylim(-10000, 200000)\n",
    "plt.legend()\n",
    "\n",
    "plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(nbins=6))\n",
    "plt.gca().yaxis.set_major_locator(ticker.MaxNLocator(nbins=6))\n",
    "\n",
    "\n",
    "# Affichage des sous-tracés\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prédictions sur la base de test standardisée\n",
    "predictions_test = modele_regression.predict(X_test)\n",
    "\n",
    "# Calcul des résidus\n",
    "residus = y_test - predictions_test\n",
    "\n",
    "# Création d'une figure avec deux sous-graphiques\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 8))\n",
    "\n",
    "# Histogramme des résidus\n",
    "axes[0].hist(residus, bins=100, edgecolor='black')\n",
    "axes[0].set_title('Histogramme des Résidus')\n",
    "axes[0].set_xlabel('Résidus')\n",
    "axes[0].set_ylabel('Fréquence')\n",
    "\n",
    "# Courbe de densité des résidus\n",
    "sns.kdeplot(residus, fill=True, ax=axes[1])\n",
    "axes[1].set_title('Courbe de Densité des Résidus')\n",
    "axes[1].set_xlabel('Résidus')\n",
    "axes[1].set_ylabel('Densité')\n",
    "\n",
    "# Ajustement de l'espace entre les sous-graphiques\n",
    "plt.tight_layout()\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation des hypothèses du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calcul des résidus\n",
    "residus = y_test - predictions\n",
    "\n",
    "# Statistiques descriptives sur les résidus\n",
    "mean_residus = np.mean(residus)\n",
    "median_residus = np.median(residus)\n",
    "std_dev_residus = np.std(residus)\n",
    "min_residus = np.min(residus)\n",
    "max_residus = np.max(residus)\n",
    "\n",
    "# Affichage des statistiques descriptives\n",
    "print(f\"Moyenne des résidus : {mean_residus}\")\n",
    "print(f\"Médiane des résidus : {median_residus}\")\n",
    "print(f\"Écart-type des résidus : {std_dev_residus}\")\n",
    "print(f\"Minimum des résidus : {min_residus}\")\n",
    "print(f\"Maximum des résidus : {max_residus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpréter les statistiques descriptives des résidus peut fournir des informations importantes sur la qualité de votre modèle de régression. Voici comment interpréter certaines de ces informations :\n",
    "\n",
    "    #Moyenne des résidus :\n",
    "        #Si la moyenne des résidus est proche de zéro, cela suggère que le modèle ne présente pas de biais systématique. Cependant, il est toujours important d'examiner d'autres aspects.\n",
    "\n",
    "    #Médiane des résidus :\n",
    "        #La médiane peut être moins sensible aux valeurs aberrantes que la moyenne. Une médiane proche de zéro indique également une absence de biais systématique.\n",
    "\n",
    "    #Écart-type des résidus :\n",
    "        #L'écart-type des résidus mesure la dispersion des résidus autour de la moyenne. Une valeur faible suggère que les résidus sont généralement proches de la moyenne, ce qui est souhaitable. Une valeur élevée peut indiquer une grande variabilité des erreurs de prédiction.\n",
    "\n",
    "    #Minimum et maximum des résidus :\n",
    "        #Examiner le minimum et le maximum des résidus peut vous aider à identifier les valeurs aberrantes ou les observations qui ont une influence disproportionnée sur votre modèle. Si vous avez des valeurs extrêmement élevées ou basses, cela pourrait indiquer des problèmes.\n",
    "\n",
    "#En résumé, des résidus centrés autour de zéro avec une dispersion modérée, et sans valeurs aberrantes évidentes, indiquent généralement un bon ajustement du modèle. Cependant, n'oubliez pas que l'interprétation dépend du contexte spécifique de votre analyse et des exigences de votre problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La visualisation de la prédiction nous laisse de fortes raisons de croire que certaines hypothèse de régression linéaire ne sont pas validé. On le vérifie par des tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import shapiro, bartlett, anderson\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Supposons que vous ayez déjà ajusté votre modèle de régression et obtenu les résidus (residus)\n",
    "# modele_regression = sm.OLS(y, X).fit()\n",
    "# residus = modele_regression.resid\n",
    "\n",
    "# Test de normalité (Shapiro-Wilk)\n",
    "stat_shapiro, p_shapiro = shapiro(residus)\n",
    "print(f\"Test de normalité (Shapiro-Wilk): Statistique={stat_shapiro}, p-value={p_shapiro}\")\n",
    "\n",
    "# Test d'indépendance (Durbin-Watson)\n",
    "stat_dw = durbin_watson(residus)\n",
    "print(f\"Test d'indépendance (Durbin-Watson): Statistique={stat_dw}\")\n",
    "\n",
    "# Test d'homoscédasticité (Bartlett)\n",
    "# Note : Le test de Bartlett teste l'homoscédasticité en supposant que les échantillons suivent une distribution normale.\n",
    "# Si vos résidus ne suivent pas une distribution normale, utilisez le test de Levene.\n",
    "stat_bartlett, p_bartlett = bartlett(residus, np.arange(len(residus)))\n",
    "print(f\"Test d'homoscédasticité (Bartlett): Statistique={stat_bartlett}, p-value={p_bartlett}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test de normalité (Shapiro-Wilk) : la p-value étant de 0.0, on rejette l'hypothèse nulle. Ainsi, les résidus ne suivent pas une distribution normale.\n",
    "\n",
    "#Test d'indépendance (Durbin-Watson) : La statistique de Durbin - Watson est très proche de 2, on suppose une très faible autocorrélation des résidus\n",
    "\n",
    "#Test d'homoscédasticité (Bartlett ou Levene) : la p-value étant de 0.0, on rejette l'hypothèse nulle. Ainsi, les résidus sont hétéroscédastiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ème idée : comparer une batterie de modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install statsmodels\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_donnees_pred=table_donnees.drop(['code_commune', 'libelle_commune', 'grp_tmp'],axis=1)\n",
    "\n",
    "# Séparation des variables indépendantes et dépendantes\n",
    "y = table_donnees_pred['consototale']\n",
    "X = table_donnees_pred.drop(['consototale'], axis=1)\n",
    "\n",
    "# Séparation en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dictionnaire pour stocker les erreurs des modèles\n",
    "model_errors = {}\n",
    "\n",
    "# Modèles à tester\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"Support Vector Regression\": SVR(),\n",
    "    \"Neural Network\": MLPRegressor(max_iter=1000)  # Augmentation du nombre d'itérations pour une meilleure convergence\n",
    "}\n",
    "\n",
    "# Entraînement et évaluation de chaque modèle\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    model_errors[name] = mse\n",
    "    print(f\"{name}: MSE = {mse}\")\n",
    "\n",
    "# Trouver le modèle avec l'erreur la plus faible\n",
    "best_model = min(model_errors, key=model_errors.get)\n",
    "print(f\"\\nMeilleur modèle: {best_model} avec une MSE de {model_errors[best_model]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ème idée : retour sur les régressions linéaires pour l'interprétabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode exhaustive de sélection de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation des données pour différentes combinaisons\n",
    "def transform_data(df, transformation):\n",
    "    transformed_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        # Vérifier si la colonne est numérique avant d'appliquer une transformation\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if transformation == \"log\" and df[col].min() > 0:\n",
    "                transformed_df[col] = np.log(df[col])\n",
    "            elif transformation == \"squared\":\n",
    "                transformed_df[col] = np.square(df[col])\n",
    "    return transformed_df\n",
    "\n",
    "# Sélection exhaustive des variables pour la régression linéaire avec enregistrement de tous les modèles\n",
    "def best_feature_combination(table_donnees, target_column):\n",
    "    best_aic = float('inf')\n",
    "    best_bic = float('inf')\n",
    "    best_combination = None\n",
    "    best_transformation = None\n",
    "    features = [col for col in table_donnees.columns if col != target_column]\n",
    "    all_models = []  # Liste pour stocker les informations de tous les modèles\n",
    "\n",
    "    for transformation in [\"log\", \"squared\", \"none\"]:\n",
    "        transformed_table = transform_data(table_donnees, transformation) if transformation != \"none\" else table_donnees\n",
    "\n",
    "        for L in range(1, len(features) + 1):\n",
    "            for subset in combinations(features, L):\n",
    "                X = transformed_table[list(subset)]\n",
    "                y = transformed_table[target_column]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "                model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "                aic = model.aic\n",
    "                bic = model.bic\n",
    "\n",
    "                # Ajouter les informations du modèle actuel à all_models\n",
    "                all_models.append({\n",
    "                    \"features\": subset,\n",
    "                    \"transformation\": transformation,\n",
    "                    \"aic\": aic,\n",
    "                    \"bic\": bic\n",
    "                })\n",
    "\n",
    "                if aic < best_aic and bic < best_bic:\n",
    "                    best_aic = aic\n",
    "                    best_bic = bic\n",
    "                    best_combination = subset\n",
    "                    best_transformation = transformation\n",
    "\n",
    "    return best_combination, best_transformation, best_aic, best_bic, all_models\n",
    "\n",
    "best_features, best_transformation, best_aic, best_bic, all_models = best_feature_combination(table_donnees_pred, 'consototale')\n",
    "\n",
    "# Affichage des meilleurs résultats\n",
    "print(f\"Meilleures caractéristiques: {best_features}\")\n",
    "print(f\"Meilleure transformation: {best_transformation}\")\n",
    "print(f\"Meilleur AIC: {best_aic}\")\n",
    "print(f\"Meilleur BIC: {best_bic}\")\n",
    "\n",
    "# Affichage de tous les modèles testés\n",
    "for model in all_models:\n",
    "    print(f\"Caractéristiques: {model['features']}, Transformation: {model['transformation']}, AIC: {model['aic']}, BIC: {model['bic']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats descriptives des variables identifiées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des graphiques\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Histogrammes\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, col in enumerate(['nombre_entreprises', 'niveau_de_vie', 'population', '']):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.histplot(table_donnees_pred[col], kde=True)\n",
    "    plt.title(f'Histogramme de {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagrammes de dispersion\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, col in enumerate(['nombre_entreprises', 'niveau_de_vie', 'population']):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.scatterplot(data=table_donnees_pred, x=col, y='consototale')\n",
    "    plt.title(f'{col} vs consototale')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap de corrélation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(table_donnees_pred.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Matrice de corrélation')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle sélectionné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, Latex, HTML\n",
    "\n",
    "\n",
    "# Ignorer les avertissements pour une sortie propre\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fonction pour transformer les données\n",
    "def transform_data(df, transformation):\n",
    "    transformed_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if transformation == \"log\" and df[col].min() > 0:\n",
    "                transformed_df[col] = np.log(df[col])\n",
    "            elif transformation == \"squared\":\n",
    "                transformed_df[col] = np.square(df[col])\n",
    "    return transformed_df\n",
    "\n",
    "# Transformation des données\n",
    "transformed_data = transform_data(table_donnees_pred, 'log')\n",
    "\n",
    "# Sélectionner les meilleures caractéristiques pour le modèle\n",
    "X = transformed_data[['nombre_entreprises', 'niveau_de_vie', 'population']]\n",
    "y = transformed_data['consototale']\n",
    "\n",
    "# Entraînement du modèle de régression linéaire\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "\n",
    "# Récupération des coefficients\n",
    "coefficients = model.params\n",
    "intercept = coefficients[0]\n",
    "coef = coefficients[1:]\n",
    "\n",
    "# Formatage de l'équation de régression pour l'affichage en LaTeX\n",
    "model_eq = r\"$$\\text{consototale} = \"\n",
    "model_eq += f\"{intercept:.2f} \"\n",
    "for var, beta in zip(['nombre_entreprises', 'niveau_de_vie', 'population'], coef):\n",
    "    sign = '+' if beta >= 0 else ''\n",
    "    model_eq += f\" {sign} {beta:.2f} \\log({var}) \"\n",
    "model_eq += r\"$$\"\n",
    "\n",
    "print(\"Modèle de régression linéaire :\")\n",
    "display(Latex(model_eq))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Préparation des données avec la meilleure combinaison et transformation\n",
    "transformed_table = transform_data(table_donnees, best_transformation)\n",
    "X = transformed_table[list(best_features)]\n",
    "y = transformed_table['consototale']\n",
    "\n",
    "# Séparation en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement du modèle\n",
    "model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "\n",
    "# Affichage des résultats du modèle\n",
    "print(model.summary())\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = model.predict(sm.add_constant(X_test))\n",
    "\n",
    "# Visualisation des résidus\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={'color': 'red', 'lw': 1})\n",
    "plt.title('Résidus vs Valeurs Prédites')\n",
    "plt.xlabel('Valeurs Prédites')\n",
    "plt.ylabel('Résidus')\n",
    "plt.show()\n",
    "\n",
    "# Diagramme de dispersion pour montrer les prédictions par rapport aux valeurs réelles\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)  # ligne de référence\n",
    "plt.title('Valeurs Réelles vs Prédites')\n",
    "plt.xlabel('Valeurs Réelles')\n",
    "plt.ylabel('Valeurs Prédites')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Une petite application de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application textuelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_consototale(population, niveau_de_vie, nombre_entreprises, model):\n",
    "    # Transformation des entrées\n",
    "    log_population = np.log(population)\n",
    "    log_niveau_de_vie = np.log(niveau_de_vie)\n",
    "    log_nombre_entreprises = np.log(nombre_entreprises)\n",
    "    \n",
    "    # Préparation des données pour la prédiction\n",
    "    X_pred = np.array([[1, log_nombre_entreprises, log_niveau_de_vie, log_population]])  # Ajoutez '1' pour la constante\n",
    "    \n",
    "    # Prédiction en utilisant le modèle\n",
    "    prediction = model.predict(X_pred)[0]\n",
    "    return prediction\n",
    "\n",
    "# Assurez-vous que 'model' est votre modèle entraîné\n",
    "# model = ...\n",
    "\n",
    "# Demande de saisie des valeurs à l'utilisateur\n",
    "population = float(input(\"Entrez la population: \"))\n",
    "niveau_de_vie = float(input(\"Entrez le niveau de vie: \"))\n",
    "nombre_entreprises = float(input(\"Entrez le nombre d'entreprises: \"))\n",
    "\n",
    "# Prédiction de la consototale\n",
    "consototale_estimee = predict_consototale(population, niveau_de_vie, nombre_entreprises, model)\n",
    "\n",
    "print(f\"La consototale estimée est : {consototale_estimee:.1f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def predict_consototale(population, niveau_de_vie, nombre_entreprises, model):\n",
    "    log_population = np.log(population)\n",
    "    log_niveau_de_vie = np.log(niveau_de_vie)\n",
    "    log_nombre_entreprises = np.log(nombre_entreprises)\n",
    "    X_pred = np.array([[1, log_nombre_entreprises, log_niveau_de_vie, log_population]])\n",
    "    prediction = model.predict(X_pred)[0]\n",
    "    return prediction\n",
    "\n",
    "def on_predict(b):\n",
    "    try:\n",
    "        population = float(population_input.value)\n",
    "        niveau_de_vie = float(niveau_de_vie_input.value)\n",
    "        nombre_entreprises = float(nombre_entreprises_input.value)\n",
    "        consototale_estimee = predict_consototale(population, niveau_de_vie, nombre_entreprises, model)\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            print(f\"La consototale estimée est : {consototale_estimee}\")\n",
    "    except ValueError as e:\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            print(\"Erreur : Veuillez entrer des valeurs numériques valides.\")\n",
    "\n",
    "population_input = widgets.FloatText(value=0, description='Population:')\n",
    "niveau_de_vie_input = widgets.FloatText(value=0, description='Niv de vie:')\n",
    "nombre_entreprises_input = widgets.FloatText(value=0, description='Nb Etp:')\n",
    "\n",
    "predict_button = widgets.Button(description=\"Prédire\")\n",
    "predict_button.on_click(on_predict)\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "display(population_input, niveau_de_vie_input, nombre_entreprises_input, predict_button, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Synthèse et Recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions Clés "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommandations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
